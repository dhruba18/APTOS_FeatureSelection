{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88a10038-d72c-43dd-815b-a31bab2844b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler  # Added for feature scaling\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b7fe0136-0e7d-430b-a5ab-a2d8416c7c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pca(x_train, x_test, n_components):\n",
    "    pca = PCA(n_components=n_components, svd_solver='full')\n",
    "    pca.fit(x_train)\n",
    "    x_train = pca.transform(x_train)\n",
    "    x_test = pca.transform(x_test)\n",
    "    print(x_train.shape, x_test.shape)\n",
    "    return x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58f5799c-ac66-4868-8b78-d3c5bc39dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantized_pso_feature_selection(csv_path, target_column=-1, n_particles=20, \n",
    "                                   max_iter=100, inertia=0.7, cognitive=1.5, \n",
    "                                   social=1.5, penalty_factor=0.01):\n",
    "    \"\"\"\n",
    "    Quantized PSO implementation for feature selection with enhanced capabilities\n",
    "    \n",
    "    Parameters:\n",
    "    csv_path (str): Path to CSV file\n",
    "    target_column (int): Index of target column (default last)\n",
    "    n_particles (int): Number of particles in swarm\n",
    "    max_iter (int): Maximum iterations\n",
    "    inertia (float): Inertia weight\n",
    "    cognitive (float): Cognitive coefficient \n",
    "    social (float): Social coefficient\n",
    "    penalty_factor (float): Feature count penalty multiplier\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (selected_features, best_score, convergence_curve)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Drop rows with null values\n",
    "    df_cleaned = df.dropna()\n",
    "    \n",
    "    # Check for null values\n",
    "    null_values = df_cleaned.isnull().sum()\n",
    "    \n",
    "    # Display the null value counts\n",
    "    print(\"Null value counts:\")\n",
    "    print(null_values)\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Apply label encoding to categorical columns\n",
    "    categorical_columns = ['id_code']\n",
    "    for column in categorical_columns:\n",
    "            df_cleaned[column] = label_encoder.fit_transform(df_cleaned[column])\n",
    "    \n",
    "\n",
    "    #X = df_cleaned.iloc[:, :target_column].values\n",
    "    #y = df_cleaned.iloc[:, target_column].values\n",
    "    X = df_cleaned.drop('label', axis=1).values  # Feature matrix\n",
    "    y = df_cleaned['label'].values  # Target variable\n",
    "   \n",
    "    # Split data with stratification\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Feature scaling for distance-based algorithms\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    temp_train_data = X_train\n",
    "    temp_test_data = X_test\n",
    "    #temp_train_data, temp_test_data = get_pca(temp_train_data, temp_test_data, 0.98)\n",
    "    # Initialize PSO parameters\n",
    "    n_features = temp_train_data.shape[1]\n",
    "    velocity_range = [-6, 6]  # Sigmoid saturation bounds\n",
    "    \n",
    "    # Initialize swarm positions and velocities\n",
    "    particles = np.random.randint(2, size=(n_particles, n_features))\n",
    "    velocities = np.random.uniform(*velocity_range, (n_particles, n_features))\n",
    "    \n",
    "    # Initialize personal and global bests\n",
    "    personal_best = particles.copy()\n",
    "    personal_scores = np.array([_evaluate(p, temp_train_data, temp_test_data, y_train, y_test, penalty_factor) for p in particles])\n",
    "    global_best_idx = np.argmax(personal_scores)\n",
    "    global_best = particles[global_best_idx].copy()\n",
    "    global_score = personal_scores[global_best_idx]\n",
    "    \n",
    "    convergence = np.zeros(max_iter)\n",
    "    \n",
    "    # PSO optimization loop\n",
    "    for iteration in range(max_iter):\n",
    "        for i in range(n_particles):\n",
    "            # Update velocity with clamping\n",
    "            r1, r2 = np.random.rand(2, n_features)\n",
    "            velocities[i] = (inertia * velocities[i] +\n",
    "                            cognitive * r1 * (personal_best[i] - particles[i]) +\n",
    "                            social * r2 * (global_best - particles[i]))\n",
    "            \n",
    "            # Apply velocity clamping\n",
    "            velocities[i] = np.clip(velocities[i], *velocity_range)\n",
    "            \n",
    "            # Update position using sigmoid probability\n",
    "            prob = 1 / (1 + np.exp(-velocities[i]))\n",
    "            particles[i] = (np.random.rand(n_features) < prob).astype(int)\n",
    "            \n",
    "            # Evaluate and update bests\n",
    "            current_score = _evaluate(particles[i],temp_train_data, temp_test_data, y_train, y_test, penalty_factor)\n",
    "            if current_score > personal_scores[i]:\n",
    "                personal_best[i] = particles[i].copy()\n",
    "                personal_scores[i] = current_score\n",
    "                \n",
    "                if current_score > global_score:\n",
    "                    global_best = particles[i].copy()\n",
    "                    global_score = current_score\n",
    "        \n",
    "        convergence[iteration] = global_score\n",
    "        print(f\"Iter {iteration+1}/{max_iter} | Best Score: {global_score:.4f}\")\n",
    "    \n",
    "    # Final feature selection\n",
    "    selected_features = np.where(global_best == 1)[0]\n",
    "    return selected_features, global_score, convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475ed4d1-1d3d-4df0-9878-cf31738c4833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _evaluate(particle, X_train, X_test, y_train, y_test, penalty_factor):\n",
    "    \"\"\"Enhanced evaluation function with cross-validation\"\"\"\n",
    "    selected = particle.astype(bool)\n",
    "    if not np.any(selected):\n",
    "        return 0\n",
    "    \n",
    "    # Reduced feature subset\n",
    "    X_train_sub = X_train[:, selected]\n",
    "    X_test_sub = X_test[:, selected]\n",
    "    \n",
    "    # Classifier with cross-validation\n",
    "    clf = KNeighborsClassifier(n_neighbors=5)\n",
    "    clf.fit(X_train_sub, y_train)\n",
    "    \n",
    "    #clf=SVC(C = 19,kernel='linear')\n",
    "    #clf.fit(X_train_sub, y_train)\n",
    "    acc = accuracy_score(y_test, clf.predict(X_test_sub))\n",
    "    \n",
    "    # Penalty term calculation\n",
    "    penalty = penalty_factor * (np.sum(particle) / particle.size)\n",
    "    return acc - penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35a3e4c0-d147-42c7-9d12-108cb449f092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null value counts:\n",
      "id_code               0\n",
      "effb3_feature_0       0\n",
      "effb3_feature_1       0\n",
      "effb3_feature_2       0\n",
      "effb3_feature_3       0\n",
      "                     ..\n",
      "effb3_feature_1532    0\n",
      "effb3_feature_1533    0\n",
      "effb3_feature_1534    0\n",
      "effb3_feature_1535    0\n",
      "label                 0\n",
      "Length: 1538, dtype: int64\n",
      "Iter 1/10 | Best Score: 0.4027\n",
      "Iter 2/10 | Best Score: 0.4027\n",
      "Iter 3/10 | Best Score: 0.4027\n",
      "Iter 4/10 | Best Score: 0.4027\n",
      "Iter 5/10 | Best Score: 0.4027\n",
      "Iter 6/10 | Best Score: 0.4027\n",
      "Iter 7/10 | Best Score: 0.4027\n",
      "Iter 8/10 | Best Score: 0.4034\n",
      "Iter 9/10 | Best Score: 0.4045\n",
      "Iter 10/10 | Best Score: 0.4047\n",
      "Selected features: [   0    1    2    4    6    7    8    9   10   11   12   13   14   15\n",
      "   20   21   22   23   24   25   30   31   32   33   36   41   47   49\n",
      "   52   54   58   66   68   70   72   77   80   81   84   88   89   95\n",
      "   99  101  102  103  104  106  107  109  110  112  113  114  117  118\n",
      "  119  121  123  128  129  130  131  132  135  136  138  139  140  141\n",
      "  142  143  145  149  152  155  156  157  158  159  162  163  165  166\n",
      "  169  170  174  175  177  178  181  183  188  190  191  192  194  197\n",
      "  198  199  200  203  204  205  206  207  208  211  212  214  216  217\n",
      "  219  220  221  222  223  224  225  229  231  233  236  237  238  243\n",
      "  244  245  246  249  250  252  253  257  259  261  262  263  264  265\n",
      "  269  277  278  279  283  285  286  287  290  292  294  299  300  303\n",
      "  305  308  309  311  312  313  316  317  318  321  326  328  331  332\n",
      "  333  334  336  337  339  340  343  344  348  350  351  352  353  354\n",
      "  356  357  358  359  361  362  363  368  371  372  373  374  375  376\n",
      "  380  381  382  386  390  391  393  397  398  401  403  404  405  408\n",
      "  409  410  412  413  416  417  420  421  422  423  425  426  427  428\n",
      "  430  432  434  435  436  437  439  440  443  444  446  447  449  450\n",
      "  451  455  457  458  459  461  462  464  465  466  476  477  478  482\n",
      "  483  485  486  487  493  497  499  500  503  504  507  508  510  514\n",
      "  516  517  521  525  526  527  528  529  532  537  540  542  543  544\n",
      "  545  546  547  548  550  551  558  560  561  562  567  570  571  572\n",
      "  573  575  576  577  578  582  585  586  588  589  590  591  593  600\n",
      "  604  605  606  607  608  609  610  612  613  615  616  619  622  623\n",
      "  628  629  630  631  633  636  637  638  645  652  654  662  668  669\n",
      "  670  674  675  677  685  686  687  688  690  693  694  695  696  698\n",
      "  699  700  701  703  705  707  708  709  710  713  714  715  716  717\n",
      "  719  727  728  729  731  734  735  738  739  740  741  742  744  746\n",
      "  748  751  753  754  756  760  761  762  767  768  769  774  775  776\n",
      "  777  778  779  781  782  785  786  787  788  789  791  793  794  796\n",
      "  799  800  801  802  803  810  812  816  818  820  821  828  834  835\n",
      "  836  837  838  840  842  845  846  847  850  851  853  854  857  860\n",
      "  861  868  870  871  872  873  874  875  877  878  880  881  883  885\n",
      "  886  887  888  891  892  897  904  912  913  914  918  921  926  929\n",
      "  930  931  932  933  935  937  938  943  946  947  950  951  955  957\n",
      "  961  965  966  973  976  977  979  981  983  984  987  994  995  998\n",
      "  999 1000 1002 1004 1005 1007 1008 1012 1014 1015 1017 1019 1021 1022\n",
      " 1023 1025 1026 1029 1031 1032 1034 1035 1036 1037 1038 1041 1042 1043\n",
      " 1045 1048 1051 1052 1054 1056 1057 1058 1059 1062 1063 1065 1067 1070\n",
      " 1072 1073 1078 1079 1080 1081 1082 1085 1086 1089 1090 1091 1093 1100\n",
      " 1101 1102 1106 1108 1111 1112 1114 1116 1117 1119 1121 1125 1126 1127\n",
      " 1133 1135 1136 1138 1139 1142 1143 1144 1145 1146 1147 1149 1150 1152\n",
      " 1153 1154 1156 1157 1158 1159 1160 1163 1165 1166 1169 1170 1173 1175\n",
      " 1176 1183 1185 1186 1187 1188 1190 1193 1194 1195 1196 1200 1201 1202\n",
      " 1203 1205 1208 1209 1213 1214 1215 1219 1220 1222 1223 1224 1226 1229\n",
      " 1230 1231 1233 1238 1239 1241 1242 1243 1244 1245 1246 1249 1250 1251\n",
      " 1253 1254 1255 1259 1260 1261 1262 1266 1269 1270 1271 1273 1275 1277\n",
      " 1278 1280 1281 1285 1289 1290 1291 1294 1296 1300 1305 1311 1313 1317\n",
      " 1319 1323 1324 1328 1329 1331 1335 1337 1338 1344 1345 1351 1352 1353\n",
      " 1354 1356 1358 1359 1360 1365 1366 1373 1375 1378 1380 1382 1383 1385\n",
      " 1386 1387 1389 1390 1391 1392 1394 1395 1396 1399 1402 1403 1404 1405\n",
      " 1407 1410 1412 1414 1418 1420 1423 1425 1427 1428 1429 1430 1431 1433\n",
      " 1435 1437 1440 1443 1447 1448 1450 1451 1452 1454 1457 1460 1461 1465\n",
      " 1468 1469 1473 1474 1475 1477 1478 1482 1483 1484 1485 1486 1489 1490\n",
      " 1491 1494 1495 1496 1497 1500 1503 1504 1506 1510 1511 1512 1513 1514\n",
      " 1517 1521 1523 1524 1525 1527 1528 1529 1530 1531 1535]\n",
      "Validation score: 0.4047\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    #file_path = 'featureoptimization/featureoptimization/densenet201_lbp_histog_feats.csv'\n",
    "    #file_path = 'featureoptimization/featureoptimization/densenet201feat.csv'\n",
    "    file_path = 'featureoptimization/featureoptimization/efficientnetb3newfeat.csv'\n",
    "    #file_path = 'featureoptimization/featureoptimization/combined_DIEnet_features.csv'\n",
    "    #file_path = 'featureoptimization/featureoptimization/inceptionv4newfeat.csv'\n",
    "    features, score, curve = quantized_pso_feature_selection(\n",
    "        \n",
    "        file_path,\n",
    "        n_particles=30,\n",
    "        max_iter=10,\n",
    "        penalty_factor=0.015\n",
    "    )\n",
    "    print(f\"Selected features: {features}\")\n",
    "    print(f\"Validation score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b186eb8e-25a5-4cc7-aac1-f29ca363d261",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1222"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "634e8fc4-eb0c-4ed8-af98-241bd31a7a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of Y :(16925,)\n",
      "shape of X :(16925, 767)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('featureoptimization/featureoptimization/efficientnetb3newfeat.csv')\n",
    "df = df[df.columns[1:]]\n",
    "df.isnull().sum().max()\n",
    "#understanding the predicted value - which is hot encoded, in real life price won't be hot encoded.\n",
    "df['label'].describe(), df['label'].unique()\n",
    "\n",
    "# there are 4 classes in the predicted value\n",
    "y_t = np.array(df['label'])\n",
    "X_t = df = df.iloc[:, features]\n",
    "#X_t = df.drop(['label'],axis=1)\n",
    "X_t = np.array(X_t)\n",
    "print(\"shape of Y :\"+str(y_t.shape))\n",
    "print(\"shape of X :\"+str(X_t.shape))\n",
    "scaler = MinMaxScaler()\n",
    "X_t = scaler.fit_transform(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "317a6fe5-6e07-48fb-8993-f38c731d7b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of X Train :(13540, 767)\n",
      "shape of X Test :(3385, 767)\n",
      "shape of Y Train :(13540,)\n",
      "shape of Y Test :(3385,)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test = train_test_split(X_t,y_t,test_size=.20,random_state=42)\n",
    "print(\"shape of X Train :\"+str(X_train.shape))\n",
    "print(\"shape of X Test :\"+str(X_test.shape))\n",
    "print(\"shape of Y Train :\"+str(Y_train.shape))\n",
    "print(\"shape of Y Test :\"+str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a239ef89-0593-4cab-b2f6-054e38d329c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13540, 646) (3385, 646)\n"
     ]
    }
   ],
   "source": [
    "#temp_train_data, temp_test_data = get_pca(X_train, X_test, 0.98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb7ce2-e685-4461-9996-275d45e3a5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-All Accuracy: 0.43751846381093057\n"
     ]
    }
   ],
   "source": [
    "for this_C in [1,2,3,4,5,6,7,8,9,10,11,12,13,17,18,19,20]:\n",
    "    svm_ova = SVC(C = this_C,kernel='rbf', decision_function_shape='ovr')\n",
    "    svm_ova.fit(X_train, Y_train)\n",
    "    y_pred_ova = svm_ova.predict(X_test)\n",
    "    print(\"One-vs-All Accuracy:\", accuracy_score(Y_test, y_pred_ova))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc104418-ccfb-4341-a575-66b2b047ba6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
